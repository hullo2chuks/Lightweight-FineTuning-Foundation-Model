{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "\n",
    "#### PEFT technique: \n",
    "LoRA is the PEFT technique used. This is because LoRA is compatible with all models at least for now. Also the LoRA is qite easy to finetune and most impotantly only a small pretrained size model is created from the foundation model, hence good space optimization\n",
    "#### Model: \n",
    "I have decided to use the `distilbert-base-uncased` foundation model. The preference for this is because it's just eppropriate and good and classicication which is the intention here\n",
    "\n",
    "#### Evaluation approach:\n",
    "A simple method passed to the trainer and used for evaluation\n",
    "\n",
    "#### Fine-tuning dataset: \n",
    "The dataset was preprocessed basically by tokenization and save dataset was used for training both the lora model and the none lora model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 20\n",
      "    })\n",
      "    validate: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 980\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Import the datasets and transformers packages\n",
    "import torch\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import (set_seed)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "splits = [\"train\", \"test\"]\n",
    "dataset = {split: ds for split, ds in zip(splits, load_dataset(\"imdb\", split=splits))}\n",
    "\n",
    "for split in splits:\n",
    "    dataset[split] = dataset[split].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "# Reserve 0.2 percent for inference(test)\n",
    "# and 0.98 for evaluation(validation)\n",
    "val_test_ds = dataset[\"test\"].train_test_split(test_size=0.02)\n",
    "dataset = DatasetDict(\n",
    "    train=dataset[\"train\"],\n",
    "    test=val_test_ds[\"test\"],\n",
    "    validate=val_test_ds[\"train\"]\n",
    ")\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2026e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "feature_field = \"text\"\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "batch_size = 4\n",
    "num_epochs = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#tokenizer.padding_side = \"left\"\n",
    "#tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenizer_fn(x):\n",
    "    return tokenizer(x[feature_field], padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = {}\n",
    "for split in [\"train\", \"validate\"]:\n",
    "    tokenized_dataset[split] = dataset[split].map(tokenizer_fn, batched=True)\n",
    "    \n",
    "    \n",
    "tokenized_train_dataset=tokenized_dataset[\"train\"]\n",
    "tokenized_eval_dataset=tokenized_dataset[\"validate\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d9011f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"}  # For converting predictions to strings\n",
    "label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Create foundation model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.to(device)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6cbe7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 00:38, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.405580</td>\n",
       "      <td>0.821429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=0.55659375, metrics={'train_runtime': 39.0389, 'train_samples_per_second': 25.615, 'train_steps_per_second': 6.404, 'total_flos': 132467398656000.0, 'train_loss': 0.55659375, 'epoch': 1.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "\n",
    "training_args=TrainingArguments(\n",
    "        output_dir=f\"./data/{model_name}_trained\",\n",
    "        learning_rate=2e-3,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_epochs,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ee8ac34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='245' max='245' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [245/245 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.405579537153244,\n",
       " 'eval_accuracy': 0.8214285714285714,\n",
       " 'eval_runtime': 16.8084,\n",
       " 'eval_samples_per_second': 58.304,\n",
       " 'eval_steps_per_second': 14.576,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,331,716 || all params: 67,694,596 || trainable%: 1.967241225577297\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS, inference_mode=False, r=16, lora_alpha=32, lora_dropout=0.05,\n",
    "    target_modules=[\"q_lin\"]\n",
    ")\n",
    "_model = AutoModelForSequenceClassification.from_pretrained(model_name,\n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "_model.to(device)\n",
    "_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "lora_model = get_peft_model(_model, peft_config)\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d48de565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 00:54, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.391402</td>\n",
       "      <td>0.851020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=0.48859368896484373, metrics={'train_runtime': 54.4246, 'train_samples_per_second': 18.374, 'train_steps_per_second': 4.594, 'total_flos': 134739406848000.0, 'train_loss': 0.48859368896484373, 'epoch': 1.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "        output_dir=f\"./data/{model_name}_peft_finetuned\",\n",
    "        learning_rate=2e-3,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_epochs,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "trainer2 = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bae7846",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer2.save_model(f\"{model_name}-peft-lora\") # or lora_model.save_pretrained(\"gpt2-peft-lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "894046c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='245' max='245' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [245/245 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.39140161871910095,\n",
       " 'eval_accuracy': 0.8510204081632653,\n",
       " 'eval_runtime': 17.3411,\n",
       " 'eval_samples_per_second': 56.513,\n",
       " 'eval_steps_per_second': 14.128,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer2.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): DistilBertForSequenceClassification(\n",
       "      (distilbert): DistilBertModel(\n",
       "        (embeddings): Embeddings(\n",
       "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "          (position_embeddings): Embedding(512, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (transformer): Transformer(\n",
       "          (layer): ModuleList(\n",
       "            (0-5): 6 x TransformerBlock(\n",
       "              (attention): MultiHeadSelfAttention(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (q_lin): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (ffn): FFN(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (activation): GELUActivation()\n",
       "              )\n",
       "              (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pre_classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=2, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To load a PEFT model for inference:\n",
    "import torch\n",
    "\n",
    "peft_model_id = f\"{model_name}-peft-lora\"\n",
    "from peft import PeftConfig, AutoPeftModelForSequenceClassification\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "inference_model = AutoPeftModelForSequenceClassification.from_pretrained(peft_model_id)\n",
    "inference_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a09709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc329d93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64515479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddaea466",
   "metadata": {},
   "source": [
    "### Manual Evaluation with unknown test data\n",
    "\n",
    "Note that same metrics and dataset is used for training both models. Also same unknown test data is used for this evaluation of both model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07475955",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_reviews_count = 20\n",
    "reviews = dataset[\"test\"][\"text\"][-infer_reviews_count:]\n",
    "labels = dataset[\"test\"][\"label\"][-infer_reviews_count:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2de08ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_prediction(review, pred_model):\n",
    "    \"\"\"Given a review, return the predicted sentiment\"\"\"\n",
    "    pred_model.to(device)\n",
    "    inputs = tokenizer(review, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "    inputs.to(device)\n",
    "    outputs = pred_model(**inputs)\n",
    "\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "    return id2label[predictions.item()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54e70c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) Review: I watched this movie which I really thought had a promising beginning but then i \n",
      "... ess when it comes to controversial matters, weirdness and originality in movies.\n",
      "Actual Label: ---------    NEGATIVE\n",
      "Original Model prediction: NEGATIVE âœ…\n",
      "Peft Model prediction      NEGATIVE âœ…\n",
      "\n",
      "(2) Review: This movie is perfect for any aspiring screen writer, actor or director. By watc \n",
      "... idering to watch this movie so they can go do something decent with their lives.\n",
      "Actual Label: ---------    NEGATIVE\n",
      "Original Model prediction: NEGATIVE âœ…\n",
      "Peft Model prediction      NEGATIVE âœ…\n",
      "\n",
      "(3) Review: I'm a Boorman fan, but this is arguably his least successful film. Comedy has ne \n",
      "... orman wrote the script with his daughter, Telsche, who died a couple years ago.)\n",
      "Actual Label: ---------    NEGATIVE\n",
      "Original Model prediction: POSITIVE ðŸ˜¡\n",
      "Peft Model prediction      POSITIVE ðŸ˜¡\n",
      "\n",
      "(4) Review: I wonder why I haven't heard of this movie before. It's truly a magnificent come \n",
      "... acter. If you liked the Tales of the City films, you'd like this quirky charmer.\n",
      "Actual Label: ---------    POSITIVE\n",
      "Original Model prediction: POSITIVE âœ…\n",
      "Peft Model prediction      POSITIVE âœ…\n",
      "\n",
      "(5) Review: Finally! Third time lucky. This film has been always been on my mind, but my fir \n",
      "... macks you with an almighty wallop when it changes direction. Highly recommended.\n",
      "Actual Label: ---------    POSITIVE\n",
      "Original Model prediction: NEGATIVE ðŸ˜¡\n",
      "Peft Model prediction      POSITIVE âœ…\n",
      "\n",
      "(6) Review: Bloody awful! There's just no other way to put it. In fact, it's **SO** bad that \n",
      "... y yourself (and Hyams) to blame because you've been more than adequately warned.\n",
      "Actual Label: ---------    NEGATIVE\n",
      "Original Model prediction: NEGATIVE âœ…\n",
      "Peft Model prediction      NEGATIVE âœ…\n",
      "\n",
      "(7) Review: There are so very few films where just the title tells you all you need to know  \n",
      "... ive work based on a real life character and a testament to those paranoid times.\n",
      "Actual Label: ---------    NEGATIVE\n",
      "Original Model prediction: NEGATIVE âœ…\n",
      "Peft Model prediction      POSITIVE ðŸ˜¡\n",
      "\n",
      "(8) Review: Chaplin is a doughboy in his final film of 1918, a doughboy who can not seem to  \n",
      "... thing he would again do to greater effect in The Great Dictator. *** of 4 stars.\n",
      "Actual Label: ---------    POSITIVE\n",
      "Original Model prediction: POSITIVE âœ…\n",
      "Peft Model prediction      POSITIVE âœ…\n",
      "\n",
      "(9) Review: \"Black Dragons\" is a second feature WWII propaganda film popular at the time. It \n",
      "...  Although a little dated now, \"Black Dragons\" is not a bad way to spend an hour.\n",
      "Actual Label: ---------    NEGATIVE\n",
      "Original Model prediction: POSITIVE ðŸ˜¡\n",
      "Peft Model prediction      POSITIVE ðŸ˜¡\n",
      "\n",
      "(10) Review: By Randolph Scott standards of the 1950s, this is a disappointing and heavy-hand \n",
      "... es the action well - as always - but his grasp of the overall narrative is weak.\n",
      "Actual Label: ---------    NEGATIVE\n",
      "Original Model prediction: POSITIVE ðŸ˜¡\n",
      "Peft Model prediction      NEGATIVE âœ…\n",
      "\n",
      "(11) Review: THE PROTECTOR. You hear the name. You think, \"ah, it's a crappy Hong Kong movie. \n",
      "... rove he could make a better cop film, Chan made the amazing POLICE STORY (1985).\n",
      "Actual Label: ---------    NEGATIVE\n",
      "Original Model prediction: NEGATIVE âœ…\n",
      "Peft Model prediction      NEGATIVE âœ…\n",
      "\n",
      "(12) Review: This is the last episode of the Goldenboy OVA series. Kentaro finds himself work \n",
      "...  time (which i do if i have a lunch break at school.) good series, check it out.\n",
      "Actual Label: ---------    POSITIVE\n",
      "Original Model prediction: NEGATIVE ðŸ˜¡\n",
      "Peft Model prediction      POSITIVE âœ…\n",
      "\n",
      "(13) Review: \"Chips\" is an excellent blend of music, light comedy and drama with a picture pe \n",
      "... e end, Chips realizes he was able to do it - but only cause Catherine was there.\n",
      "Actual Label: ---------    POSITIVE\n",
      "Original Model prediction: POSITIVE âœ…\n",
      "Peft Model prediction      POSITIVE âœ…\n",
      "\n",
      "(14) Review: After reading the mostly glowing comments about this movie I decided to rent it  \n",
      "... n save one person from wasting 100 minutes on this tripe I will feel vindicated.\n",
      "Actual Label: ---------    NEGATIVE\n",
      "Original Model prediction: NEGATIVE âœ…\n",
      "Peft Model prediction      NEGATIVE âœ…\n",
      "\n",
      "(15) Review: One of the many backwoods horror's that came out in the early eightes and fortun \n",
      "... or ending with the final girl and the killer it's totally not what you'd expect.\n",
      "Actual Label: ---------    POSITIVE\n",
      "Original Model prediction: NEGATIVE ðŸ˜¡\n",
      "Peft Model prediction      POSITIVE âœ…\n",
      "\n",
      "(16) Review: Well, the episode I just watched had the older \"Gastineau Girl\" whining about wh \n",
      "... ney, and all it tells you about rich people is that they have no money problems.\n",
      "Actual Label: ---------    NEGATIVE\n",
      "Original Model prediction: NEGATIVE âœ…\n",
      "Peft Model prediction      NEGATIVE âœ…\n",
      "\n",
      "(17) Review: This TV show is possibly the most pathetic display of crap on TV today. Horribly \n",
      "... ill prefer it over reality TV, it can't really get any more worthless than that.\n",
      "Actual Label: ---------    NEGATIVE\n",
      "Original Model prediction: NEGATIVE âœ…\n",
      "Peft Model prediction      NEGATIVE âœ…\n",
      "\n",
      "(18) Review: The two leads, an Englishman and an Aussie filming an American Civil War story i \n",
      "... row in some spinning, whirling, kung fu jumping off horses. What was that about?\n",
      "Actual Label: ---------    NEGATIVE\n",
      "Original Model prediction: NEGATIVE âœ…\n",
      "Peft Model prediction      NEGATIVE âœ…\n",
      "\n",
      "(19) Review: I remember seeing this film in the mid 80's thought it a well paced and well act \n",
      "... se be a great film but I guess is in line with the caper tag the film goes with.\n",
      "Actual Label: ---------    POSITIVE\n",
      "Original Model prediction: POSITIVE âœ…\n",
      "Peft Model prediction      POSITIVE âœ…\n",
      "\n",
      "(20) Review: NBC should be ashamed. I wouldn't allow my children to see this. I definitely wo \n",
      "... the story in Genesis. How could anyone bring this to any screen, small or large!\n",
      "Actual Label: ---------    NEGATIVE\n",
      "Original Model prediction: NEGATIVE âœ…\n",
      "Peft Model prediction      NEGATIVE âœ…\n",
      "\n",
      "---Manual Inference Using same unknown test data---\n",
      "Original Model: Correctly predicted 14 out of 20\n",
      "Peft Model: Correctly predicted 17 out of 20\n"
     ]
    }
   ],
   "source": [
    "def add_emoji(is_true: bool) -> str:\n",
    "    return 'âœ…' if is_true else 'ðŸ˜¡' \n",
    "\n",
    "original_model_correct_predtions_count = 0\n",
    "peft_model_correct_prediction_count = 0\n",
    "total_lebel_count = len(labels)\n",
    "counter = 0\n",
    "for review, label in zip(reviews, labels):\n",
    "    counter += 1\n",
    "    print(f\"({counter}) Review: {review[:80]} \\n... {review[-80:]}\")\n",
    "    #Peft Trained model prediction\n",
    "    peft_model_prediction = get_prediction(review, inference_model)\n",
    "    is_peft_model_predict_correct = label2id[peft_model_prediction] == label\n",
    "    if is_peft_model_predict_correct:\n",
    "        peft_model_correct_prediction_count += 1\n",
    "\n",
    "    # Original trained model prediction\n",
    "    original_model_prediction = get_prediction(review, model)\n",
    "    is_original_model_predict_correct = label2id[original_model_prediction] == label\n",
    "    if is_original_model_predict_correct:\n",
    "        original_model_correct_predtions_count += 1\n",
    "\n",
    "          \n",
    "    print(f\"Actual Label: ---------    {id2label[label]}\\n\"\n",
    "          f\"Original Model prediction: {original_model_prediction} {add_emoji(is_original_model_predict_correct)}\\n\"\n",
    "          f\"Peft Model prediction      {peft_model_prediction} {add_emoji(is_peft_model_predict_correct)}\\n\")\n",
    "\n",
    "print(\"---Manual Inference Using same unknown test data---\")\n",
    "print(f\"Original Model: Correctly predicted {original_model_correct_predtions_count} out of {total_lebel_count}\")\n",
    "print(f\"Peft Model: Correctly predicted {peft_model_correct_prediction_count} out of {total_lebel_count}\")\n",
    "                     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21fc9f9",
   "metadata": {},
   "source": [
    "#### Inference from the Peft Trained LoRA model vs the original Loaded model\n",
    "\n",
    "From the manual evaluation above, the perf model performed better much better. Using 20 sample unknown test data, the peft model predicted 17 out of 20 reviews correctly. But the original model trained without peft LoRA predicted only 14 out of 20 correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fceb84f",
   "metadata": {},
   "source": [
    "### Inference Training Evaluation\n",
    "Note that same metrics and dataset is used for training both models. Also same unknown test data is used for this evaluation of both model.\n",
    "\n",
    "With the appropriate hyper-parameter set for the lora-model during training for the peft model, the peft model performed better during the training evaluation.\n",
    "\n",
    "##### Originally trained model result\n",
    "```json\n",
    "{'eval_loss': 0.405579537153244,\n",
    " 'eval_accuracy': 0.8214285714285714,\n",
    " 'eval_runtime': 16.8084,\n",
    " 'eval_samples_per_second': 58.304,\n",
    " 'eval_steps_per_second': 14.576,\n",
    " 'epoch': 1.0}\n",
    " ```\n",
    " \n",
    " ####W Peft trained model with LoRA result\n",
    "```json\n",
    "{'eval_loss': 0.39140161871910095,\n",
    " 'eval_accuracy': 0.8510204081632653,\n",
    " 'eval_runtime': 17.3411,\n",
    " 'eval_samples_per_second': 56.513,\n",
    " 'eval_steps_per_second': 14.128,\n",
    " 'epoch': 1.0}\n",
    " ```\n",
    "\n",
    "It's evident from the above the accuracy `eval_accuracy` of the peft trained model with LoRA is `0.85` againt `0.82` for the original loaded model. Though not too big but signaficant.\n",
    "\n",
    "Also the `eval_loss` for the peft trained model is also 0.39 which is small than 0.41 for the original model. This also indicates better performance of the peft model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36368bba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
